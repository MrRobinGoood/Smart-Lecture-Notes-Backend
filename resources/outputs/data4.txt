you you you you Добрый день, дорогие слушатели! Я очень рад, что вы пришли. Надеюсь, что вы узнаете много полезного из этого небольшого учебного курса по переносу обучения. У нас будет три лекции и три семинара. Мы будем учиться применять современные большие нейросети в продуктовых приложениях. Меня зовут Николай Копылин, я инженер и преподаватель в инженерном центре Экспонента. В 2009 я окончил магистратуру французского вуза Анссьета по сложным техническим системам. В 2010-м получил диплом специалиста Московского авиационного института. Отучился в аспирантуре, работал в нескольких компаниях инженерного профиля, продолжая все время преподавать. Мне повелось тем, что я не раз занимался вам сказать, что машинное обучение очень востребовано везде. В этой сфере есть заказчики, есть специализированные инженеры. Даже инженеры других профилей за год делают несколько проектов с применением нейросетей и прочих ML-алгоритмов. Кстати, сокращение ML означает Machine Learning. Я попробую его использовать пореже, но не могу совсем исключить, особенно в семинарах, которые у нас будут, и там общение более спонтанно. Пару слов о предварительных знаниях. Тему переноса обучения можно осваивать как до того, как вы пройдете нейросети, так и после того. Для нас эти конкретные предобученные нейросети будут являться одним лишь компонентом продукта. Возможно, мы немного зарезем в терминологию и детали работы нейросетей, но это будет не специально. Попробуем это сделать так, чтобы у вас осталось много сил на внедрение нейросетей в продукт. Если вам уже сейчас хочется сформировать себе интуицию о работе нейросетей, например, представить, какое решение заведомо имеет много трудностей или низкую вероятность срабатывания, то я рекомендую обратиться к соответствующему курсу, где изучаются глубокие нейросети. Для нас с вами, для нашего курса достаточно будет общей математической интуиции, которую можно воспитать в ходе изучения любой дисциплины – информатики, программирования, моделирования систем, информационной безопасности, математики, просто линейной алгебры, не помешает курс по классическому машинному обучению, куда входят разные алгоритмы, предшествующие нейросетям. У нас в курсе будет три лекции и три семинара. На первой лекции мы поговорим о методе переноса машинного обучения. Такой возможности выполнять вычисления заранее, помещать их в контейнер, чтобы использовать потом. Если необходимо, этот контейнер можно будет немножко дообучить. Как это делать, вы научитесь на первом занятии, в ходе лекции и семинара. Вторая лекция будет посвящена использованию внутреннего состояния предобученных нейросетей. Эти замечательные алгоритмы можно использовать частично, чтобы преобразовывать наши с вами цифровые объекты, делая их более пригодными к автоматизированному анализу. На семинаре мы изучим, как пользоваться внутренними представлениями для задачи переноса стиля. И на семинаре мы с вами посмотрим, как пользоваться внутренними представлениями нейросетей для того, чтобы решать полезные задачи. На третьей лекции мы пройдем много разных применений переноса обучения. На третьей лекции мы с вами пройдем много разных применений переноса обучения. А в финале реализуем большую задачу, где соединим несколько разных приемов. Я надеюсь, что большую задачу, где соединим несколько разных приемов. Я надеюсь, что из этого курса вы вынесете для себя знания о том, какие популярные предобученные нейросети используются в промышленности, как на их основе построить собственный продукт. Вы сможете идентифицировать нужную вам нейросеть, скачать ее, запустить в вашем проекте, а используя базовые навыки программирования, вы сможете из выводов этой нейросети сделать ваши собственные, нужные вам выводы и автоматизировать самые трудные когнитивные задачи. Это наша первая лекция в курсе, и она будет посвящена методу переноса обучения, который позволяет извлекать максимум пользы из моделей, которые вы обучаете на основе данных. В некотором смысле это метод продуктивизации обученных моделей. Возможно, вы уже проходили машинное обучение, наверняка слышали пару раз про нейросети в принципе. В этой части курса не обязательно помнить математику и не обязательно знать устройство алгоритмов, которые обучают нейросети. Достаточно общих идей. Например, того, что модели в машинном обучении создаются на основе данных. Возможно, вы уже проходили классическое машинное обучение и наверняка слышали пару вещей про нейросети в принципе. В этой части курса не обязательно помнить математику, знать устройство алгоритмов. Достаточно общих идей. Например, того, как в цифровом виде представляются картинки или текст. И то, что модели в машинном обучении вообще создаются на основе данных. И то, что модели в машинном обучении вообще создаются на основе данных, но не представляют из себя каталогов по этим данным. Они извлекают из них правила принятия решений для какой-то практической задачи. По нейросетям достаточно будет помнить, что нейросети обычно организованы в несколько слоев. Очень пригодится способность воплощать решение задач в программах, то есть ваши навыки программирования, и склонность к работе с данными, то есть вам должно быть удобно работать с таблицами, получать из них разные строки и столбцы, но конечно в нашем коде будет дано много примеров, и вам не придется реализовывать эти задачи самостоятельно с нуля. Перенос обучения, как и машинное обучение, в принципе, преследует задачу воплотить человеческий интеллект в программе. Мы пока не так хорошо оперируем другими формами мышления, зато нам хорошо понятен такой ход обучения. Возмож возможность посмотреть на несколько объектов, держа в уме конкретную задачу. Потом извлечь из ряда объектов какое-то абстрактное правило и проверить его на других объектах. Люди тоже ничему не учатся с нуля. Перед тем, как научиться говорить на иностранном языке, мы обычно осваиваем грамматику родного языка. Перед тем, как научиться различать породы собак или кошек, мы прежде учимся различать эти виды животных, проводя некоторую мысленную линию между ними. Уже пройденный опыт складывается в два результата, которые у нас остаются в мышлении. Во-первых, готовые правила принятия решений, благодаря которым мы можем мыслить детективно, то есть принимать решения. Во-вторых, наблюдательность, то есть мы учимся обращать внимание на те вещи, на которые раньше бы не посмотрели, то есть использовать более сложные признаки входных объектов для принятия решений. В общем, при обсуждении переноса обучения мы довольно близки к человеческому мышлению. Даже если модель машинного обучения будет для нас полностью черным ящиком, то есть мы не будем знать, что внутри, все равно при таком интуитивном взгляде на интеллектуальные алгоритмы, вы будете лучше представлять, что в них поменять и вообще как их использовать, чтобы с их помощью аппроксимировать человеческое мышление. В этом модуле вы научитесь использовать предобученные, уже готовые нейросети для преобразования информации и принятия решений. Задачи будут требовать от вас довольно стандартных навыков программирования и подготовки рабочего пространства. Загрузки файлов из интернета, открытия, выводы графиков. Все это базовый курс программирования. По плану занятий на первой лекции мы обсудим метод переноса обучения, с помощью которого сегодня решается очень много прикладных задач. Поговорим о его истории и развитии. Посмотрим, что это за задача, которую он так хорошо решает, а также какие манипуляции с нейросетями нужно проделать, чтобы адаптировать обученную сеть под собственную задачу. Потом будет семинар для закрепления навыков, мы будем скачивать нейросети и добиваться от них решения наших задач. А на второй лекции мы коснемся той самой вышеупомянутой наблюдательности нейросетей, то есть их способности формировать какую-то интуитивную картину внутри, преобразовывать задачу в более удобную форму, обращать внимание на более глубокие признаки. Вторая лекция будет касаться фичер инжиниринга или инженерии признаков. В третьей лекции мы будем объединять все это вместе и строить более сложные методы на основе двух приемов, которые изучим на первых лекциях. Мы хотим, чтобы вы, как слушатели, научились конкретным вещам. Использованию современного глубокого обучения в в ваших проектах. Научились собирать короткий код с применением готовых нейросетей. Научились понимать и объяснять процесс создания таких алгоритмов, так чтобы вы смогли выступать в роли квалифицированных пользователей или даже квалифицированных заказчиков. Я постараюсь указывать исторические источники и документы, чтобы вам всегда было где узнать больше. В конце лекции будет идти библиография, где я постараюсь указать все исторические источники и документы, чтобы вы могли прочитать побольше о каждой теме. Вы можете пользоваться переводчиком или читать в оригинале, как вам удобно. Тема переноса обучения очень живо развивается, и в каждой популярной статье на эту тему есть какие-то намеки, которые не раз направляли подготовку этого курса в новом интересном направлении. Тема переноса обучения очень живо развивается. И в каждой популярной статье на эту тему есть какие-то намеки, которые не раз меняли направление подготовки этого курса, давая ему новое интересное направление. Итак, давайте приступим. Искусственный интеллект пытаются создать довольно давно. Если в 50-х годах все проблемы казались небольшими и разрешимыми, то спустя несколько циклов упадка финансирования таких разработок, вы их видите, их называют зимой искусственного интеллекта, решено было относиться более скептически к созданию думающих машин. Понятие искусственного интеллекта развивалось от имитаторов, которым можно было программировать вручную, через так называемые символьные машины, до современного Data Science, включающего методы классического и глубокого машинного обучения и в том числе наш с вами перенос обучения. Но финальная цель, к которой мы стремимся, состоит в том, чтобы можно было обучить алгоритм, который будет полезен во множестве когнитивных задач сразу. В каком-то плане он должен быть подобен человеку. И при этом, поскольку это технический продукт, мы сможем компоновать из него что-то новое, эксплуатировать промежуточные результаты, так сказать, залезать в его интуицию. В общем, хотелось бы не создавать новый интеллект под каждую маленькую новую задачу. Правда в том, что пока что часто складывается так, что когда аналитик берется за новую задачу, данных для ее решения все еще очень мало. Или они есть, но их нужно преобразовывать в форму, удобную для алгоритмов, то есть заниматься их статистическими параметрами. Удалять данные, которые мы не хотим учитывать, искать и исключать предрассудки, и побочные зависимости, паразитные зависимости, их называют ликами, то есть утечками данных. В общем, для новых задач у нас обычно мало данных. Перенос обучения призван решить и эту проблему. Основная задача переноса обучения состоит в том, чтобы взять одну уже обученную нейросеть, которая обучалась в некотором домене объектов на некотором одном дата сете это будет наша исходная сеть так называемая back bone или upstream сеть будет наша исходная сеть back bone или upstream как ее только не называют есть всякие другие названия и мы будем использовать эту предобученную сеть в другой целевой таргетные или downstream задачи вот разные термины которые вы встретите в публикациях. При переходе на новый набор данных, этой предобученной модели, может понадобиться адаптация. Если исходная модель, например, тренировалась предсказывать возраст по паспортным фотографиям лиц, а вы планируете ее использовать для идентификации сотрудников, скажем, чтобы система знала, кто резервирует какой-нибудь переговорный кабинет, то ваши портреты не будут так же хорошо освещены. Может быть, даже не будут фронтальными, в отличие от паспортных фотографий. Нужно немного переобучить систему, чтобы она учитывала новое окружение. Это называется доменной адаптацией. Мы говорим сегодня о том, как это все применяется в компьютерном зрении. Ведь нам, в принципе, хотелось бы обучать алгоритмы, которые преобразуют изображения в какие-то абстрактные числа, на базе которых мы уже можем делать аналитику. Например, нам нужно определить, что же именно изображено в кадре. Это задача классификации. Сегодня она по большей части решена. А вот задача детекции, где именно находится интересующий нас объ на субъект и находите ли вы его вообще в кадре? Или задача распознавания текста? Все это требует комбинации из нескольких предобученных нейросетей и других алгоритмов. А что если мы хотели бы обучить модель общим принципам распознавания изображений на огромном датасете? Например, просто на любых фотографиях из интернета. А потом научить ее определять возраст человека на фотографии. Или то, насколько нашей аудитории понравится та или иная фотография. Или определять то, насколько нашей аудитории понравится та или иная фотография. Последняя задача очень субъективная, но на ней отлично демонстрируется перенос обучения между схожими задачами. Во приборке текстов нам бы хотелось, чтобы нейросеть выучила взаимосвязи концепции языка независимо от того, какой это язык. В нашем десятилетии лингвистические модели в чат-ботах уже не удивляют. Эти модели появились где-то в 16-18 годах, они были гораздо проще, чем текущие варианты GPT. Например, нейросеть BERT. Её можно было использовать для грамматического анализа. Для чего только ее не использовали. Для грамматического анализа, для перевода с языка на язык, для той же классификации текстов. И сейчас эта сеть не потеряла интереса в небольших задачах анализа, хотя прогресс GPT показывает, что для создания чат-бота нужно было значительно доработать тогда принятый подход. Итак, изображение и текст. изображение и текст. Эти две области не обобщают все машинное обучение. Итак, мы поговорили об изображении и тексте. Эти две области очень наглядные, но это еще не все машинное обучение. Есть очень много задач продуктовой аналитики, работы с таблицами, графами, создание рекомендательных систем. Здесь очень помогают те самые абстрактные промежуточные представления, которые нейросеть может сделать из картинок или текстов. Так делаются и чат-боты, и классификаторы компаний на честные и мошеннические. На основе анализа графов взаимосвязей и текстов их сообщений. Это замечательная техника, позволяющая вам использовать данные из одной задачи, чтобы помочь вам решить вашу задачу. Это одна из техник, которые очень часто используются в продуктовом машинном обучении. Давайте посмотрим, как работает перенос обучения. Допустим, вы хотите распознать цифры от 0 до 9. Но у вас нет большого количества размеченных данных об этих рукописных цифрах. Для обучения эффективной модели такого количества данных может быть совсем недостаточно. Что мы можем предпринять в таком случае? Допустим, вы нашли очень большой набор данных из нескольких миллионов изображений с фотографиями собак, машин, кошек, людей и так далее. Всего 1000 разных классов. Что сделает аналитик данных в таком сценарии? Он может сперва выполнить обучение нейросети на том наборе данных, который у него уже есть, который включает тысячу различных классов. Он обучит алгоритм принимать на вход изображение размером x на y и возвращать в ответ в качестве результата один из тысяч разных классов. Точнее говоря, возвращать много распределений вероятности наблюдать каждой из тысячи классов. А потом выбрать самый вероятный исход. Дальше, наверное, вы уже слышали, что самые интересные нейросети это глубокие нейросети, те, в которых информация проходит через много слоев. И так вы обучаете первый слой, второй и так далее до слоя, предшествующего выходному. В этом процессе вы находите параметры для каждого слоя нейросети. Первого слоя, второго и так далее, до десятого, до сотого. Когда сеть обучена, мы используем ее в процессе transfer learning. Применение переноса обучения заключается в том, чтобы взять копию этой нейронной сети, в которой сохраняются все слои вплоть до девятого, допустим, а последний десятый слой мы исключаем, отрезаем и заменяем на гораздо меньший выходной слой не с тысячи элементами, а только с десятью. То есть мы уже будем искать другое вероятностное распределение и по его максимуму судить, какой класс какое число преобладает, почему это должно работать. В нейросетях, которые обычно приводят в пример по этим задачам, данные проходят от входов к выходам в одну сторону. Каждый следующий слой служит для очередного этапа преобразования данных. То есть у каждого слоя собственная отдельная функция. Их работа зависит от предыдущих слоев, да. Но в обратную сторону это не работает. Предыдущие слои не зависят от последующих. Значит, если последние слои убрать, оставшаяся часть сети все равно будет решать какую-то полезную задачу. После такой модификации, то есть убрав один или несколько слоев нейросети, мы сможем создать на выходе нейросети другую, нужную нам структуру. Несколько полносвязанных слоев и softmax после них во многом аналогичны классификатору метод ближайших соседей, который вы уже могли проходить на классическом машинном обучении. Если мы уберем классификатор на выходе и добавим на его место другой, например, по методу ближайших соседей, или с большим количеством слоев, или с меньшим, почему бы и нет, подмешивающих данные какой-нибудь еще выход в какой-нибудь другой университе, то мы можем получить на выходе что-то действительно интересное. Но возникает проблема. Мы взяли большую университь, которую кто-то, возможно, неделю, возможно, месяцы обучал. Положим, мы мне что-то поменяли. И что теперь нам тоже неделю нужно ждать, пока она обучится? А что если наступят проблемы переобучения? Ведь по условию у нас нет огромного и чистого датасета под нашу целевую задачу. Значит, в нейросети будут сотни или даже десятки примеров для обучения. Естественно, она переобучится, запомнит все эти примеры наизусть, а когда встретит новое, будет принимать случайные решения. К тому же она, скорее всего, напрочь забудет предыдущую задачу, которой ее обучали. Это не очень хорошо. Чтобы такого не случалось, слои старой сети замораживают, просят их не обращать внимание на процедуру обучения, заодно и экономят на вычислениях. Сперва можно обучать только последнюю добавленную конструкцию из парочки слоев, а затем можно разморозить сеть и на очень маленькой скорости дать ей немного дообучиться. Это один из способов увеличения качества модели. Но если он не сработает, ничего страшного, есть несколько других. Поговорим о том, чем перенос обучения отличается от классического машинного обучения. Перенос обучения нам нужен для того, чтобы создавать модели приемлемой точности, имея не так уж много данных в датасете. В классическом машинном обучении такая задача обычно считалась безнадежной. Но в случае с переносом обучения это решается. Простыми словами, перенос обучения это все еще метод машинного обучения. При работе с предобученными моделями, как я уже говорил, можно разделять модель на две части. Предобученный преобразователь и дополнительный алгоритм. Вот эта финальная часть иногда называется головой модели, а предобученная часть называется хвостом. Соглашусь, это немножко странно. Но можно считать, что именно голова принимает решение и выдает результаты интеллектуальной задачи. А справная модель позволяет очень быстро найти следующую оптимальную модель для вашей задачи на совсем небольшом количестве имеющихся у вас данных. Справная модель позволяет очень быстро найти следующую оптимальную для вашей задачи модель на совсем небольшом количестве имеющихся у вас данных. Наверное, вам интересно, может ли быть перенос обучения не на нейросетях, а на моделях классического машинного обучения? Мне такое не встречалось. В модели классического машинного обучения линейная логистическая регрессия, классификация, опорные векторы, ближайшие соседи, байосовские методы и т.д., деревья обычно не так уж долго обучаются. Их предобучение занимает десятки минут, может быть, часы. Но в этой сфере практически нет моделей, которые бы обучались неделями или месяцами. То есть аспект заморозки вычислений здесь не очень актуален. Всегда выгодно обучить модель заново. Что касается дообучения классических алгоритмов, давайте изучим вот такой простой пример. Часто у таких алгоритмов даже нет возможности дообучиться. Но предположим, что она есть, и вот мы создали такой алгоритм. Предположим, у нас есть один датасет, находится он слева, и он относится к исходной задаче. Вот эти зеленые и красные круги. И есть еще один датасет размером поменьше, который находится справа. Это наша целевая задача. На самом деле вы можете придумывать разные геометрические способы, как перенести обучение в такой задаче. Но рассмотрим автоматический, такой умозрительный пример. Допустим, мы обучили классификатор на левом датасете. И если ничто не намекает на то, что разделительная линия будет поворачивать, то мы будем считать, что она продолжится. Так, чтобы разделять и правый датасет. Так вот, наша разделительная линия для этого классификатора будет выглядеть примерно так. Если мы возьмем тот же самый классификатор и обучим его только на точках второго датасета, то вы видите, что эта линия теперь довольно плохо разделяет первый датасет. Вот в этой таблице есть результаты эксперимента для логистической регрессии. На самом деле, у большинства классических классификаторов нет возможности вообще имитировать перенос обучения. Но если для нашего линейного классификатора выбрать решатель, который работает пошагово, то можно сымитировать этот процесс. Итак, если обучиться только на первом датасете, где группы немножко перекрываются, то мы получаем 94% на первом датасете, где группы немного перекрываются, то мы получаем 94% на первом датасете и 80% на втором. Что довольно неплохо, надо сказать, но для геометрической задачи можно сделать лучше. К сожалению, бизнеса этого, скорее всего, будет недостаточно, ведь в продуктовом окружении обычно мало данных и очень высокие требования по качеству. Если мы обыжим классификатор только на втором датасете, то качество на первом датасете очень сильно падает. Вы видите, что разделяющая линия практически не работает для элементов первого датасета. И если обучить классификатор на обоих датасетах сразу, то мы получаем 95% на первом и 90% на втором. Но если взять первый датасет и за один шаг чуть-чуть его дообучить, для того, чтобы он лучше воспринимал второй датасет, используя ньютоновский метод оптимизации, пошаговую оптимизацию, то мы получаем чуть-чуть другой результат. 65% на первом датасете и 90% на втором. Это очень простой пример, но среди всех интерпретаций того, что вы сейчас видели, важен такой вывод. Если полностью заново повторить обучение на новом датасете, классификатор забудет все, что он знал про исходную выборку, включая все интересные взаимосвязи между элементами, которые вполне могли бы быть полезны и новому классификатору. Ведь вполне вероятно, что в реальной жизни нам попадутся образцы, находящиеся между этими двумя выборками. И мы только что построили несколько методов, которые не очень хорошо решают эту задачу. Когда мы скачиваем нейросеть и предобучаем её под свою задачу или просто используем её